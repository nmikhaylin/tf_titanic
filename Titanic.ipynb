{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string(\"summaries_dir\", \"summaries\", \"Directory for summaries.\")\n",
    "\n",
    "DATA_FOLDER = \"/media/nikita/BigData/projects/tf_projects/titanic/data\"\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def ExtractCabinLetter(cabin_str):\n",
    "  if cabin_str:\n",
    "    match = re.match(\"[a-zA-Z]+\", cabin_str)\n",
    "    if match:\n",
    "      return match.group(0)\n",
    "  return \"\"\n",
    "\n",
    "def ExtractCabinNumber(cabin_str):\n",
    "  if cabin_str:\n",
    "    match = re.search(\"[0-9]+\", cabin_str)\n",
    "    if match:\n",
    "      return int(match.group(0))\n",
    "  return -1\n",
    "\n",
    "\n",
    "def LoadTestingData():\n",
    "  raw_rows = []\n",
    "  pclasses = set()\n",
    "  genders = set()\n",
    "  embarked = set()\n",
    "  cabin_letters = set()\n",
    "  cabin_numbers = set()\n",
    "  df = pd.DataFrame(\n",
    "      columns=[\"PassengerId\", \"Pclass\", \"Name\", \"Sex\", \"Age\",\n",
    "               \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin_Letter\", \"Cabin_Number\", \"Embarked\"])\n",
    "  with open(os.path.join(DATA_FOLDER, \"titanic_test.csv\"), \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "      raw_rows.append(row)\n",
    "      if not row[\"Embarked\"] in pclasses:\n",
    "          embarked.add(row[\"Embarked\"])\n",
    "      if not row[\"Sex\"] in pclasses:\n",
    "          genders.add(row[\"Sex\"])\n",
    "      if not row[\"Pclass\"] in pclasses:\n",
    "          pclasses.add(row[\"Pclass\"])\n",
    "      cabin_letter = ExtractCabinLetter(row[\"Cabin\"])\n",
    "      if not cabin_letter in cabin_letters:\n",
    "          cabin_letters.add(cabin_letter)\n",
    "      row[\"Cabin_Letter\"] = cabin_letter\n",
    "      cabin_number = ExtractCabinNumber(row[\"Cabin\"])\n",
    "      if not cabin_number in cabin_numbers:\n",
    "          cabin_numbers.add(cabin_number)\n",
    "      row[\"Cabin_Number\"] = cabin_number\n",
    "      row[\"Parch\"] = int(row[\"Parch\"])\n",
    "      row[\"Fare\"] = float(row[\"Parch\"])\n",
    "      del row[\"Cabin\"]\n",
    "      df.loc[row[\"PassengerId\"]] = pd.Series(row)\n",
    "  return df\n",
    "\n",
    "\n",
    "def LoadTrainingData():\n",
    "  raw_rows = []\n",
    "  pclasses = set()\n",
    "  genders = set()\n",
    "  embarked = set()\n",
    "  cabin_letters = set()\n",
    "  cabin_numbers = set()\n",
    "  df = pd.DataFrame(\n",
    "      columns=[\"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\",\n",
    "               \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin_Letter\", \"Cabin_Number\", \"Embarked\"])\n",
    "  with open(os.path.join(DATA_FOLDER, \"titanic_train.csv\"), \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "      raw_rows.append(row)\n",
    "      if not row[\"Embarked\"] in pclasses:\n",
    "          embarked.add(row[\"Embarked\"])\n",
    "      if not row[\"Sex\"] in pclasses:\n",
    "          genders.add(row[\"Sex\"])\n",
    "      if not row[\"Pclass\"] in pclasses:\n",
    "          pclasses.add(row[\"Pclass\"])\n",
    "      cabin_letter = ExtractCabinLetter(row[\"Cabin\"])\n",
    "      if not cabin_letter in cabin_letters:\n",
    "          cabin_letters.add(cabin_letter)\n",
    "      row[\"Cabin_Letter\"] = cabin_letter\n",
    "      cabin_number = ExtractCabinNumber(row[\"Cabin\"])\n",
    "      if not cabin_number in cabin_numbers:\n",
    "          cabin_numbers.add(cabin_number)\n",
    "      row[\"Cabin_Number\"] = cabin_number\n",
    "      row[\"Parch\"] = int(row[\"Parch\"])\n",
    "      row[\"Fare\"] = float(row[\"Parch\"])\n",
    "      del row[\"Cabin\"]\n",
    "      df.loc[row[\"PassengerId\"]] = pd.Series(row)\n",
    "  return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  training_df = LoadTrainingData()\n",
    "  testing_df = LoadTestingData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_df = training_df.drop(\"Survived\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MakePredictions(training_df, testing_df):\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    training_labels = training_df[\"Survived\"].astype(np.int32)\n",
    "    training_features = ProcessFeatures(training_df.drop(\"Survived\", 1))\n",
    "    print training_features\n",
    "    testing_features = ProcessFeatures(testing_df)\n",
    "    labels = training_labels.values\n",
    "    one_hotted_labels = pd.get_dummies(labels).as_matrix()\n",
    "    feature_size = training_features.shape[1]\n",
    "    print \"Number of features: %d\" % feature_size\n",
    "    input_features = tf.placeholder(tf.float32, shape=[None, feature_size])\n",
    "    output_labels = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    # Output is survived or not.\n",
    "    W = tf.Variable(tf.zeros([feature_size, 2]))\n",
    "    b = tf.Variable(tf.zeros([2]))\n",
    "    pred = tf.nn.softmax(tf.matmul(input_features, W) + b)\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(pred, output_labels))\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "    print \"Initial cost: %f\" % cost.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(one_hotted_labels,1), tf.argmax(pred,1)), tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + \"/train\", sess.graph)\n",
    "    summary = sess.run(merged, feed_dict={input_features: training_features,\n",
    "                                    output_labels: one_hotted_labels})\n",
    "    train_writer.add_summary(summary)\n",
    "    print \"Initial Accuracy %f\" % accuracy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    train_step.run(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Final cost: %f\" % cost.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Final Accuracy %f\" % accuracy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Predictions: \"\n",
    "    predictions = tf.argmax(pred, 1).eval(\n",
    "        feed_dict={input_features: testing_features})\n",
    "    print len(predictions)\n",
    "    ret_df = pd.DataFrame(data={\"PassengerId\": testing_df[\"PassengerId\"],\n",
    "                                \"Survived\": predictions})\n",
    "    print ret_df\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProcessFeatures(raw_df):\n",
    "  \"\"\"Returns a dict for the fixed tensor columns.\"\"\"\n",
    "  ret_df = raw_df.drop(\"Name\", 1)\n",
    "  ret_df = ret_df.drop(\"Ticket\", 1)\n",
    "  ret_df = ret_df.drop(\"PassengerId\", 1)\n",
    "  remove_unknowns = lambda x:  float(x) if x else 30.0\n",
    "  is_unknown = lambda x:  False if x else True\n",
    "  ret_df[\"Age_Unknown\"] = ret_df[\"Age\"].apply(is_unknown)\n",
    "  ret_df[\"Age\"] = ret_df[\"Age\"].apply(remove_unknowns)\n",
    "  ret_df[\"Fare_Unknown\"] = ret_df[\"Fare\"].apply(is_unknown)\n",
    "  ret_df[\"Fare\"] = ret_df[\"Fare\"].apply(remove_unknowns)\n",
    "  int_columns = [\"Parch\", \"SibSp\"]\n",
    "  for column in int_columns:\n",
    "      ret_df[column] = ret_df[column].astype(np.int32)\n",
    "  categorical_columns = [\n",
    "      \"Pclass\", \"Sex\", \"Cabin_Letter\", \"Embarked\", \"Fare_Unknown\",\n",
    "      \"Age_Unknown\"]\n",
    "  one_hotted = pd.get_dummies(\n",
    "      ret_df, columns=categorical_columns).astype(np.float32)\n",
    "  for col in (set([u'Cabin_Letter_',\n",
    "                   u'Cabin_Letter_A', u'Cabin_Letter_B', u'Cabin_Letter_C',\n",
    "                   u'Cabin_Letter_D', u'Cabin_Letter_E', u'Cabin_Letter_F',\n",
    "                   u'Cabin_Letter_G', u'Embarked_C', u'Cabin_Letter_T', u'Embarked_',\n",
    "                   u'Embarked_Q', u'Embarked_S'])\n",
    "              - set(one_hotted.columns)):\n",
    "      one_hotted[col] = 0.0\n",
    "  print one_hotted.describe()\n",
    "  one_hotted = one_hotted[list(sorted(one_hotted.columns))]\n",
    "  # Remove unnecessary columns from the full matrix.\n",
    "  print one_hotted.columns\n",
    "  columns_to_select = [\"Sex_male\", \"Sex_female\", \"Age\", \"Age_Unknown_True\",\n",
    "                       \"Age_Unknown_False\", \"Fare\", \"Fare_Unknown_True\",\n",
    "                       \"Fare_Unknown_False\", \"Cabin_Number\", \"Pclass_1\",\n",
    "                       \"Pclass_2\", \"Pclass_3\"]\n",
    "  return one_hotted[columns_to_select].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Age       SibSp       Parch        Fare  Cabin_Number  \\\n",
      "count  891.000000  891.000000  891.000000  891.000000    891.000000   \n",
      "mean    29.758888    0.523008    0.381594   23.209877     10.557800   \n",
      "std     13.002570    1.102744    0.806057   12.128884     27.242765   \n",
      "min      0.420000    0.000000    0.000000    1.000000     -1.000000   \n",
      "25%     22.000000    0.000000    0.000000   30.000000     -1.000000   \n",
      "50%     30.000000    0.000000    0.000000   30.000000     -1.000000   \n",
      "75%     35.000000    1.000000    0.000000   30.000000     -1.000000   \n",
      "max     80.000000    8.000000    6.000000   30.000000    148.000000   \n",
      "\n",
      "         Pclass_1    Pclass_2    Pclass_3  Sex_female    Sex_male  \\\n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean     0.242424    0.206510    0.551066    0.352413    0.647587   \n",
      "std      0.428790    0.405028    0.497665    0.477990    0.477990   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    1.000000    0.000000    1.000000   \n",
      "75%      0.000000    0.000000    1.000000    1.000000    1.000000   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "             ...         Cabin_Letter_G  Cabin_Letter_T   Embarked_  \\\n",
      "count        ...             891.000000      891.000000  891.000000   \n",
      "mean         ...               0.004489        0.001122    0.002245   \n",
      "std          ...               0.066889        0.033501    0.047351   \n",
      "min          ...               0.000000        0.000000    0.000000   \n",
      "25%          ...               0.000000        0.000000    0.000000   \n",
      "50%          ...               0.000000        0.000000    0.000000   \n",
      "75%          ...               0.000000        0.000000    0.000000   \n",
      "max          ...               1.000000        1.000000    1.000000   \n",
      "\n",
      "       Embarked_C  Embarked_Q  Embarked_S  Fare_Unknown_False  \\\n",
      "count  891.000000  891.000000  891.000000          891.000000   \n",
      "mean     0.188552    0.086420    0.722783            0.239057   \n",
      "std      0.391372    0.281141    0.447876            0.426747   \n",
      "min      0.000000    0.000000    0.000000            0.000000   \n",
      "25%      0.000000    0.000000    0.000000            0.000000   \n",
      "50%      0.000000    0.000000    1.000000            0.000000   \n",
      "75%      0.000000    0.000000    1.000000            0.000000   \n",
      "max      1.000000    1.000000    1.000000            1.000000   \n",
      "\n",
      "       Fare_Unknown_True  Age_Unknown_False  Age_Unknown_True  \n",
      "count         891.000000         891.000000        891.000000  \n",
      "mean            0.760943           0.801347          0.198653  \n",
      "std             0.426747           0.399210          0.399210  \n",
      "min             0.000000           0.000000          0.000000  \n",
      "25%             1.000000           1.000000          0.000000  \n",
      "50%             1.000000           1.000000          0.000000  \n",
      "75%             1.000000           1.000000          0.000000  \n",
      "max             1.000000           1.000000          1.000000  \n",
      "\n",
      "[8 rows x 27 columns]\n",
      "Index([u'Age', u'Age_Unknown_False', u'Age_Unknown_True', u'Cabin_Letter_',\n",
      "       u'Cabin_Letter_A', u'Cabin_Letter_B', u'Cabin_Letter_C',\n",
      "       u'Cabin_Letter_D', u'Cabin_Letter_E', u'Cabin_Letter_F',\n",
      "       u'Cabin_Letter_G', u'Cabin_Letter_T', u'Cabin_Number', u'Embarked_',\n",
      "       u'Embarked_C', u'Embarked_Q', u'Embarked_S', u'Fare',\n",
      "       u'Fare_Unknown_False', u'Fare_Unknown_True', u'Parch', u'Pclass_1',\n",
      "       u'Pclass_2', u'Pclass_3', u'Sex_female', u'Sex_male', u'SibSp'],\n",
      "      dtype='object')\n",
      "[[  1.   0.  22. ...,   0.   0.   1.]\n",
      " [  0.   1.  38. ...,   1.   0.   0.]\n",
      " [  0.   1.  26. ...,   0.   0.   1.]\n",
      " ..., \n",
      " [  0.   1.  30. ...,   0.   0.   1.]\n",
      " [  1.   0.  26. ...,   1.   0.   0.]\n",
      " [  1.   0.  32. ...,   0.   0.   1.]]\n",
      "              Age       SibSp       Parch        Fare  Cabin_Number  \\\n",
      "count  418.000000  418.000000  418.000000  418.000000    418.000000   \n",
      "mean    30.216507    0.447368    0.392345   23.645933      9.358851   \n",
      "std     12.635016    0.896760    0.981429   11.829079     24.450615   \n",
      "min      0.170000    0.000000    0.000000    1.000000     -1.000000   \n",
      "25%     23.000000    0.000000    0.000000   30.000000     -1.000000   \n",
      "50%     30.000000    0.000000    0.000000   30.000000     -1.000000   \n",
      "75%     35.750000    1.000000    0.000000   30.000000     -1.000000   \n",
      "max     76.000000    8.000000    9.000000   30.000000    132.000000   \n",
      "\n",
      "         Pclass_1    Pclass_2    Pclass_3  Sex_female    Sex_male    ...      \\\n",
      "count  418.000000  418.000000  418.000000  418.000000  418.000000    ...       \n",
      "mean     0.255981    0.222488    0.521531    0.363636    0.636364    ...       \n",
      "std      0.436934    0.416416    0.500135    0.481622    0.481622    ...       \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    ...       \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    ...       \n",
      "50%      0.000000    0.000000    1.000000    0.000000    1.000000    ...       \n",
      "75%      1.000000    0.000000    1.000000    1.000000    1.000000    ...       \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    ...       \n",
      "\n",
      "       Cabin_Letter_G  Embarked_C  Embarked_Q  Embarked_S  Fare_Unknown_False  \\\n",
      "count      418.000000  418.000000  418.000000  418.000000          418.000000   \n",
      "mean         0.002392    0.244019    0.110048    0.645933            0.224880   \n",
      "std          0.048912    0.430019    0.313324    0.478803            0.418004   \n",
      "min          0.000000    0.000000    0.000000    0.000000            0.000000   \n",
      "25%          0.000000    0.000000    0.000000    0.000000            0.000000   \n",
      "50%          0.000000    0.000000    0.000000    1.000000            0.000000   \n",
      "75%          0.000000    0.000000    0.000000    1.000000            0.000000   \n",
      "max          1.000000    1.000000    1.000000    1.000000            1.000000   \n",
      "\n",
      "       Fare_Unknown_True  Age_Unknown_False  Age_Unknown_True  Cabin_Letter_T  \\\n",
      "count         418.000000         418.000000        418.000000           418.0   \n",
      "mean            0.775120           0.794258          0.205742             0.0   \n",
      "std             0.418004           0.404727          0.404727             0.0   \n",
      "min             0.000000           0.000000          0.000000             0.0   \n",
      "25%             1.000000           1.000000          0.000000             0.0   \n",
      "50%             1.000000           1.000000          0.000000             0.0   \n",
      "75%             1.000000           1.000000          0.000000             0.0   \n",
      "max             1.000000           1.000000          1.000000             0.0   \n",
      "\n",
      "       Embarked_  \n",
      "count      418.0  \n",
      "mean         0.0  \n",
      "std          0.0  \n",
      "min          0.0  \n",
      "25%          0.0  \n",
      "50%          0.0  \n",
      "75%          0.0  \n",
      "max          0.0  \n",
      "\n",
      "[8 rows x 27 columns]\n",
      "Index([               u'Age',  u'Age_Unknown_False',   u'Age_Unknown_True',\n",
      "            u'Cabin_Letter_',     u'Cabin_Letter_A',     u'Cabin_Letter_B',\n",
      "           u'Cabin_Letter_C',     u'Cabin_Letter_D',     u'Cabin_Letter_E',\n",
      "           u'Cabin_Letter_F',     u'Cabin_Letter_G',     u'Cabin_Letter_T',\n",
      "             u'Cabin_Number',          u'Embarked_',         u'Embarked_C',\n",
      "               u'Embarked_Q',         u'Embarked_S',               u'Fare',\n",
      "       u'Fare_Unknown_False',  u'Fare_Unknown_True',              u'Parch',\n",
      "                 u'Pclass_1',           u'Pclass_2',           u'Pclass_3',\n",
      "               u'Sex_female',           u'Sex_male',              u'SibSp'],\n",
      "      dtype='object')\n",
      "Number of features: 12\n",
      "Initial cross_entropy: 0.693146\n",
      "Initial Accuracy 0.616162\n",
      "Final cross_entropy: 0.685611\n",
      "Final Accuracy 0.627385\n",
      "Predictions: \n",
      "418\n",
      "     PassengerId  Survived\n",
      "892          892         0\n",
      "893          893         0\n",
      "894          894         0\n",
      "895          895         0\n",
      "896          896         0\n",
      "897          897         0\n",
      "898          898         0\n",
      "899          899         0\n",
      "900          900         0\n",
      "901          901         0\n",
      "902          902         0\n",
      "903          903         0\n",
      "904          904         0\n",
      "905          905         0\n",
      "906          906         0\n",
      "907          907         0\n",
      "908          908         0\n",
      "909          909         0\n",
      "910          910         0\n",
      "911          911         0\n",
      "912          912         0\n",
      "913          913         0\n",
      "914          914         0\n",
      "915          915         0\n",
      "916          916         0\n",
      "917          917         0\n",
      "918          918         0\n",
      "919          919         0\n",
      "920          920         0\n",
      "921          921         0\n",
      "...          ...       ...\n",
      "1280        1280         0\n",
      "1281        1281         0\n",
      "1282        1282         0\n",
      "1283        1283         0\n",
      "1284        1284         0\n",
      "1285        1285         0\n",
      "1286        1286         0\n",
      "1287        1287         0\n",
      "1288        1288         0\n",
      "1289        1289         0\n",
      "1290        1290         0\n",
      "1291        1291         0\n",
      "1292        1292         0\n",
      "1293        1293         0\n",
      "1294        1294         0\n",
      "1295        1295         0\n",
      "1296        1296         0\n",
      "1297        1297         0\n",
      "1298        1298         0\n",
      "1299        1299         0\n",
      "1300        1300         0\n",
      "1301        1301         0\n",
      "1302        1302         0\n",
      "1303        1303         0\n",
      "1304        1304         0\n",
      "1305        1305         0\n",
      "1306        1306         0\n",
      "1307        1307         0\n",
      "1308        1308         0\n",
      "1309        1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "  with tf.Session() as sess:\n",
    "    training_labels = training_df[\"Survived\"].astype(np.int32)\n",
    "    training_features = ProcessFeatures(training_df.drop(\"Survived\", 1))\n",
    "    print training_features\n",
    "    testing_features = ProcessFeatures(testing_df)\n",
    "    labels = training_labels.values\n",
    "    one_hotted_labels = pd.get_dummies(labels).as_matrix()\n",
    "    feature_size = training_features.shape[1]\n",
    "    print \"Number of features: %d\" % feature_size\n",
    "    input_features = tf.placeholder(tf.float32, shape=[None, feature_size])\n",
    "    output_labels = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    # Output is survived or not.\n",
    "    W = tf.Variable(tf.zeros([feature_size, 2]))\n",
    "    b = tf.Variable(tf.zeros([2]))\n",
    "    pred = tf.nn.softmax(tf.matmul(input_features, W) + b)\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(pred, output_labels))\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "    print \"Initial cross_entropy: %f\" % cross_entropy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(one_hotted_labels,1), tf.argmax(pred,1)), tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + \"/train\", sess.graph)\n",
    "    summary = sess.run(merged, feed_dict={input_features: training_features,\n",
    "                                    output_labels: one_hotted_labels})\n",
    "    train_writer.add_summary(summary)\n",
    "    print \"Initial Accuracy %f\" % accuracy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    train_step.run(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Final cross_entropy: %f\" % cross_entropy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Final Accuracy %f\" % accuracy.eval(\n",
    "        feed_dict={input_features: training_features, output_labels: one_hotted_labels})\n",
    "    print \"Predictions: \"\n",
    "    predictions = tf.argmax(pred, 1).eval(\n",
    "        feed_dict={input_features: testing_features})\n",
    "    print len(predictions)\n",
    "    ret_df = pd.DataFrame(data={\"PassengerId\": testing_df[\"PassengerId\"],\n",
    "                                \"Survived\": predictions})\n",
    "    print ret_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
